{
 "metadata": {
  "name": "",
  "signature": "sha256:aade34194445187a500977d0028d13e73a1fc2fe3911c81a91f9243f6973aead"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "#Recommender Systems\n",
      "\n",
      "Core Skills:\n",
      "\n",
      "- Understand the concept of collaborative filtering\n",
      "- Apply user-based collaborative filtering to recommend items\n",
      "- Apply item-based collaborative filtering to recommend items\n",
      "- Use item attribute data to create a hybrid recommender\n",
      "- Understand the challenges of tuning a recommender\n",
      "\n",
      "## Collaborative Filtering, and The Long Tail\n",
      "\n",
      "We live in a world where almost anything you can imagine can be consumed immediately, or delivered to your doorstep overnight. There exists media and products to fit every interest, but the explosion of content and products presents a problem of discovery. If I'm into [Goth Martha Stewart decor](http://www.trystancraft.com/martha/about.html) or [Neoclassical Dark Wave music](http://en.wikipedia.org/wiki/Neoclassical_dark_wave), how do I find new items that both fit my interests, and are of high quality?\n",
      "\n",
      "A similar problem exists with mainstream interests - if I like action/adventure novels, how do I discover a great new author that hasn't yet made it to the bestseller lists? Solving these problems not only allow people to pursue their own individual passions and to discover artists who may have gone undiscovered, but also make a lot of money for companies selling content and products. By dynamically tailoring online stores to the unique, individual interests of their customers (this is called *personalization*,) they simply sell more stuff. \n",
      "\n",
      "Collaborative filtering attempts to solve these problems using the \"wisdom of the crowds.\" If I have data on what a lot of people buy, watch, or listen to, I can mine that data to find other people with similar tastes to my own. By looking for items these other people have liked that I haven't yet discovered, I can produce compelling recommendations of new stuff to consume.\n",
      "\n",
      "**Discussion point** - When's the last time you bought or consumed something as a result of a recommender system?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A multi-dimensional problem: understanding user ratings of movies\n",
      "\n",
      "It's hard to make sense of data when there are more dimensions than you can visualize. A good example is a data set of movie ratings from individual people. The MovieLens data set, made publicly available by the GroupLens project (http://www.grouplens.org) is one such data set - thousands of users rated thousands of movies on a scale of 1 to 5 stars.\n",
      "\n",
      "One way to represent this data is by imaging a multi-dimensional space where each dimension represents the rating of an individual movie, and individual users are plotted within this space. If you wanted to find similarities between users or look for interesting patterns such as clusters of people who like particular film genres, you can't just plot the data and eyeball it. There are way more dimensions than the human brain has evolved to deal with there. So, we need some tools for managing high-dimensional data such as this.\n",
      "\n",
      "Before we dive in, let's do some housekeeping and load a small-ish sample of the MovieLens data set of movie ratings from various people. You'll need to download the \"100K\" data set from http://www.grouplens.org/node/12 for yourself, as we're not allowed to redistribute it. This data set contains 100,000 ratings from about 1000 users on about 1700 movies. This method will load up a dictionary of user ID's to movie names and their ratings for that user, as well as a dictionary of movie ID's to movie names. As a test, we'll dump out the ratings for user ID # 2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadMovieRatings():\n",
      "    numMovies = 0\n",
      "    movieNames = {}\n",
      "    for line in open('../input/ml-100k/u.item'):\n",
      "        movieData = line.split('|')\n",
      "        movieID = movieData[0]\n",
      "        movieName = movieData[1]\n",
      "        movieNames[movieID] = movieName\n",
      "        numMovies += 1\n",
      "    ratings = {}\n",
      "    for line in open('../input/ml-100k/u.data'):\n",
      "        (userID, movieID, rating, timestamp) = line.split('\\t') \n",
      "        movieName = movieNames[movieID]\n",
      "        if (not(ratings.has_key(userID))):\n",
      "            ratings[userID] = {}\n",
      "        ratings[userID][movieName] = float(rating)\n",
      "        \n",
      "    return (ratings, movieNames, numMovies)\n",
      "\n",
      "(ratings, movieNames, numMovies) = loadMovieRatings()\n",
      "print \"Our data has \" + repr(numMovies) + \" dimensions.\"\n",
      "print ratings['2']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Our data has 1682 dimensions.\n",
        "{'Fly Away Home (1996)': 4.0, 'Birdcage, The (1996)': 4.0, 'Emma (1996)': 5.0, 'Liar Liar (1997)': 1.0, \"Marvin's Room (1996)\": 3.0, 'Face/Off (1997)': 3.0, 'English Patient, The (1996)': 4.0, 'Men in Black (1997)': 4.0, 'Wings of the Dove, The (1997)': 5.0, 'Contact (1997)': 3.0, 'Bed of Roses (1996)': 3.0, 'Fargo (1996)': 5.0, 'Fierce Creatures (1997)': 3.0, 'Mighty Aphrodite (1995)': 4.0, 'Rainmaker, The (1997)': 4.0, 'Kolya (1996)': 5.0, 'Sense and Sensibility (1995)': 5.0, 'Promesse, La (1996)': 3.0, 'Midnight in the Garden of Good and Evil (1997)': 3.0, \"Antonia's Line (1995)\": 3.0, 'Good Will Hunting (1997)': 5.0, 'Postino, Il (1994)': 4.0, 'Apt Pupil (1998)': 1.0, 'Jerry Maguire (1996)': 4.0, 'Hoodlum (1997)': 4.0, 'Once Upon a Time... When We Were Colored (1995)': 4.0, 'Donnie Brasco (1997)': 4.0, 'L.A. Confidential (1997)': 5.0, 'Truth About Cats & Dogs, The (1996)': 4.0, 'Mrs. Brown (Her Majesty, Mrs. Brown) (1997)': 4.0, 'Secrets & Lies (1996)': 5.0, 'Heat (1995)': 4.0, 'Absolute Power (1997)': 3.0, \"Devil's Advocate, The (1997)\": 3.0, \"Ulee's Gold (1997)\": 4.0, 'In & Out (1997)': 4.0, \"My Best Friend's Wedding (1997)\": 4.0, 'Ice Storm, The (1997)': 3.0, 'Godfather, The (1972)': 5.0, 'Full Monty, The (1997)': 4.0, '3 Ninjas: High Noon At Mega Mountain (1998)': 1.0, 'Up Close and Personal (1996)': 3.0, 'Air Force One (1997)': 4.0, 'Scream (1996)': 3.0, 'Leaving Las Vegas (1995)': 4.0, 'Breakdown (1997)': 4.0, 'Shall We Dance? (1996)': 5.0, 'Titanic (1997)': 5.0, 'Evita (1996)': 3.0, 'FairyTale: A True Story (1997)': 3.0, 'Deceiver (1997)': 1.0, 'Restoration (1995)': 4.0, 'Rosewood (1997)': 4.0, 'Sabrina (1995)': 3.0, 'As Good As It Gets (1997)': 5.0, 'Richard III (1995)': 2.0, 'River Wild, The (1994)': 3.0, 'Tin Cup (1996)': 4.0, 'Star Wars (1977)': 5.0, 'Toy Story (1995)': 4.0, 'Time to Kill, A (1996)': 4.0}\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Distance metrics and a priori knowledge\n",
      "\n",
      "We need to define what \"nearest\" means in the algorithm above. Recall we defined several distance metrics such as Euclidian, Manhattan, and Cosine back in lecture 8 on noise. Any time we have a choice like this to make, it is an opportunity to apply a priori knowledge we may have about the data.\n",
      "\n",
      "Straight Euclidian distance might be fine, but one thing I know about humans is that some are a lot pickier than others. In the context of movie ratings, this means some users may rate everything a little bit lower or higher on average than the average user does. Let's introduce a new distance metric, the *Pearson Correleation Coefficient*, which handles this case well.\n",
      "\n",
      "Pearson correlation tells you whether two sets of data are basically pointing in the same direction. If the actual values are consistently higher or lower, that's OK - it's the trends that are important. A correlation coefficient of 1 means perfect correlation, 0 means no correlation, and -1 means an inverse correlation.\n",
      "\n",
      "The Pearson Correlation Coefficient is expressed as:\n",
      "\n",
      "$r = \\dfrac{\\sum XY - \\dfrac{\\sum X \\sum Y}{n}}{\\sqrt{\\left(\\sum X^2 - \\dfrac{(\\sum X)^2}{n}\\right)\\left(\\sum Y^2 - \\dfrac{(\\sum Y)^2}{n}\\right)}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Turning this into code is straightforward enough (I know this could be done more concisely, but I'm generally going for readability for non-Python-gurus):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pearson(X, Y):\n",
      "    n = len(X)\n",
      "    if n == 0: return 0\n",
      "    \n",
      "    sumX = 0.0\n",
      "    sumY = 0.0\n",
      "    sumX2 = 0.0\n",
      "    sumY2 = 0.0\n",
      "    sumXY = 0.0\n",
      "    \n",
      "    for i in range(n):\n",
      "        x = float(X[i])\n",
      "        y = float(Y[i])\n",
      "        sumX += x\n",
      "        sumY += y\n",
      "        sumXY += x * y\n",
      "        sumX2 += x * x\n",
      "        sumY2 += y * y\n",
      "    \n",
      "    numerator = sumXY - ( (sumX * sumY) / n )\n",
      "    denominatorTerm1 = sumX2 - ( (sumX * sumX) / n)\n",
      "    denominatorTerm2 = sumY2 - ( (sumY * sumY) / n)\n",
      "    denominator = (denominatorTerm1 * denominatorTerm2) ** 0.5\n",
      "    if (denominator == 0.0):\n",
      "        return 0.0\n",
      "    else:\n",
      "        return numerator / denominator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## User-based Collaborative Filtering\n",
      "\n",
      "- Find users similar to yourself\n",
      "- See what they consumed, and score those items somehow. We'll call these your \"recommendation candidates.\"\n",
      "- Sort the resulting list, filter out stuff you've already seen / bought / etc., and there are your recommendations.\n",
      "\n",
      "It's a simple concept, but the nuances come into play in defining similar users, and in scoring the recommendation candidates. Let's use the MovieLens data set and produce movie recommendations for individuals using it.\n",
      "\n",
      "Load up our ratings data, and measure the similarity between one user and the others."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll pick on user ID 9 for this class; let's have a peek at that user's ratings:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(ratings, movieNames, numMovies) = loadMovieRatings()\n",
      "print ratings['9']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'Boogie Nights (1997)': 4.0, 'Liar Liar (1997)': 4.0, 'Face/Off (1997)': 5.0, 'English Patient, The (1996)': 5.0, 'Star Wars (1977)': 5.0, 'Vertigo (1958)': 4.0, 'Kolya (1996)': 4.0, 'Ghost (1990)': 4.0, 'Dark City (1998)': 5.0, 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)': 5.0, 'Seven Years in Tibet (1997)': 1.0, 'Roman Holiday (1953)': 5.0, 'Gandhi (1982)': 3.0, 'Twelve Monkeys (1995)': 4.0, '39 Steps, The (1935)': 4.0, 'Deer Hunter, The (1978)': 4.0, 'Casablanca (1942)': 5.0, 'True Lies (1994)': 5.0, 'Leaving Las Vegas (1995)': 4.0, 'Bridges of Madison County, The (1995)': 5.0, 'Streetcar Named Desire, A (1951)': 4.0, 'Evil Dead II (1987)': 5.0}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we need to do is find the users that are similar to user 9, based on their ratings. This is very similar to what we did when computing K-Nearest-Neighbors, but we will go through the entire user base and compute similarities to everybody else, then sort the user base by their similarity to user 9."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findSimilarUsers(targetUserID, allRatings):\n",
      "    similarUsers = []\n",
      "    targetUserRatings = allRatings[targetUserID]\n",
      "    for user in allRatings:\n",
      "        # Exclude myself:\n",
      "        if (user != targetUserID):\n",
      "            userSimilarity = 0\n",
      "            # Find mutually rated movies (if any)\n",
      "            targetUserRatingsArray = []\n",
      "            otherUserRatingsArray = []\n",
      "            for movie in targetUserRatings:\n",
      "                if allRatings[user].has_key(movie):\n",
      "                    targetUserRatingsArray.append(\n",
      "                        float(targetUserRatings[movie]))\n",
      "                    otherUserRatingsArray.append(\n",
      "                        float(allRatings[user][movie]))\n",
      "        \n",
      "            # Compute the similarity between these users\n",
      "            userSimilarity = pearson(targetUserRatingsArray, \n",
      "                otherUserRatingsArray)\n",
      "            # If there is a positive correlation:\n",
      "            if userSimilarity > 0.0:\n",
      "                similarUsers.append((userSimilarity, user))\n",
      "    \n",
      "    # Sort the list by similarity\n",
      "    similarUsers.sort(reverse=True)\n",
      "    return similarUsers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(ratings, movieNames, numMovies) = loadMovieRatings()\n",
      "similarUsers = findSimilarUsers('9', ratings)\n",
      "print similarUsers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(1.000000000000004, '196'), (1.0000000000000002, '137'), (1.0, '941'), (1.0, '909'), (1.0, '895'), (1.0, '882'), (1.0, '88'), (1.0, '858'), (1.0, '852'), (1.0, '847'), (1.0, '826'), (1.0, '821'), (1.0, '818'), (1.0, '816'), (1.0, '800'), (1.0, '785'), (1.0, '776'), (1.0, '775'), (1.0, '768'), (1.0, '767'), (1.0, '742'), (1.0, '740'), (1.0, '720'), (1.0, '667'), (1.0, '652'), (1.0, '626'), (1.0, '585'), (1.0, '578'), (1.0, '563'), (1.0, '555'), (1.0, '550'), (1.0, '55'), (1.0, '544'), (1.0, '53'), (1.0, '517'), (1.0, '51'), (1.0, '476'), (1.0, '421'), (1.0, '415'), (1.0, '390'), (1.0, '386'), (1.0, '370'), (1.0, '366'), (1.0, '356'), (1.0, '355'), (1.0, '353'), (1.0, '351'), (1.0, '315'), (1.0, '310'), (1.0, '29'), (1.0, '283'), (1.0, '278'), (1.0, '266'), (1.0, '251'), (1.0, '243'), (1.0, '235'), (1.0, '208'), (1.0, '155'), (1.0, '149'), (1.0, '140'), (1.0, '139'), (1.0, '129'), (1.0, '101'), (0.9999999999999987, '704'), (0.9999999999999987, '238'), (0.9999999999999987, '117'), (0.9999999999999947, '633'), (0.9999999999999947, '259'), (0.970725343394154, '309'), (0.970725343394154, '284'), (0.9707253433941512, '713'), (0.970725343394151, '611'), (0.9707253433941485, '681'), (0.9707253433941485, '364'), (0.9684959969581862, '619'), (0.968245836551856, '507'), (0.9622504486493763, '803'), (0.9622504486493763, '22'), (0.9449111825230686, '248'), (0.9449111825230636, '595'), (0.9449111825230636, '41'), (0.9354143466934831, '324'), (0.9271726499455306, '943'), (0.9185586535436894, '346'), (0.9185586535436888, '709'), (0.9165444688834552, '751'), (0.9045340337332909, '715'), (0.9045340337332909, '162'), (0.8944271909999159, '839'), (0.8910421112136305, '241'), (0.8750000000000048, '763'), (0.8749999999999983, '330'), (0.8660254037844402, '912'), (0.8660254037844402, '756'), (0.8660254037844402, '725'), (0.8660254037844402, '706'), (0.8660254037844402, '274'), (0.8660254037844402, '247'), (0.866025403784439, '541'), (0.866025403784439, '426'), (0.8660254037844355, '557'), (0.8660254037844355, '352'), (0.8451542547285166, '937'), (0.8451542547285166, '654'), (0.8401680504168065, '109'), (0.8386278693775345, '179'), (0.8164965809277261, '893'), (0.8164965809277261, '780'), (0.8017837257372732, '82'), (0.8012352517785893, '433'), (0.7905694150420953, '878'), (0.7816608327818948, '799'), (0.7745966692414834, '703'), (0.7745966692414834, '294'), (0.7745966692414834, '203'), (0.7637626158259759, '908'), (0.7637626158259759, '830'), (0.7637626158259759, '642'), (0.7637626158259716, '256'), (0.7592566023652946, '848'), (0.7484551991837488, '790'), (0.7463904912524636, '236'), (0.7438343052617352, '634'), (0.7205766921228921, '431'), (0.7205766921228899, '113'), (0.7071067811865475, '942'), (0.7071067811865475, '668'), (0.7071067811865475, '596'), (0.7071067811865475, '580'), (0.7071067811865475, '579'), (0.7071067811865475, '575'), (0.7071067811865475, '466'), (0.6933752452815368, '570'), (0.6933752452815368, '400'), (0.6933752452815368, '258'), (0.6933752452815368, '147'), (0.6933752452815368, '105'), (0.6919053632356148, '526'), (0.6859943405700311, '625'), (0.6666666666666627, '621'), (0.6666666666666627, '394'), (0.6622661785325219, '773'), (0.6622661785325219, '714'), (0.6584863977663936, '85'), (0.6546536707079772, '329'), (0.6454972243679062, '102'), (0.645497224367902, '84'), (0.645497224367902, '494'), (0.645497224367902, '32'), (0.645497224367902, '25'), (0.6374552583116774, '299'), (0.6324555320336832, '540'), (0.6324555320336719, '221'), (0.6201736729460424, '601'), (0.6123724356957956, '528'), (0.6123724356957956, '177'), (0.6123724356957949, '70'), (0.6123724356957949, '290'), (0.6123724356957927, '734'), (0.6123724356957891, '350'), (0.6123724356957891, '14'), (0.6024640760767093, '119'), (0.5940885257860051, '361'), (0.5773502691896258, '96'), (0.5773502691896258, '823'), (0.5773502691896258, '781'), (0.5773502691896258, '712'), (0.5773502691896258, '68'), (0.5773502691896258, '560'), (0.5773502691896258, '552'), (0.5773502691896258, '484'), (0.5773502691896258, '383'), (0.5773502691896258, '220'), (0.5773502691896257, '65'), (0.5773502691896257, '5'), (0.5642880936468351, '629'), (0.5629262517526427, '831'), (0.5629262517526424, '553'), (0.5619514869490144, '593'), (0.5590169943749435, '200'), (0.5570860145311559, '577'), (0.5483870967741935, '682'), (0.5423261445466402, '538'), (0.5388159060803247, '864'), (0.5345224838248481, '64'), (0.5345224838248481, '24'), (0.5262348115842176, '76'), (0.5229763603684907, '213'), (0.5222329678670935, '901'), (0.5222329678670935, '670'), (0.5222329678670935, '655'), (0.5222329678670935, '491'), (0.5222329678670935, '323'), (0.5161319060481773, '806'), (0.5152098373193089, '618'), (0.5101127853361851, '632'), (0.5101127853361851, '42'), (0.509524665365063, '152'), (0.5091750772173154, '587'), (0.5041841733655171, '328'), (0.5000000000000053, '926'), (0.5000000000000053, '879'), (0.5000000000000053, '730'), (0.5000000000000053, '581'), (0.5000000000000053, '489'), (0.5000000000000053, '333'), (0.5000000000000027, '89'), (0.5000000000000027, '736'), (0.5000000000000027, '586'), (0.5000000000000027, '496'), (0.5000000000000027, '403'), (0.5000000000000027, '37'), (0.5000000000000027, '368'), (0.5000000000000027, '252'), (0.5000000000000027, '240'), (0.5000000000000027, '186'), (0.5000000000000019, '701'), (0.5000000000000019, '582'), (0.5000000000000009, '921'), (0.5000000000000009, '837'), (0.5, '686'), (0.5, '435'), (0.5, '411'), (0.5, '114'), (0.49999999999999933, '906'), (0.49999999999999933, '829'), (0.49999999999999933, '779'), (0.49999999999999933, '741'), (0.49999999999999933, '722'), (0.49999999999999933, '45'), (0.49999999999999933, '320'), (0.49999999999999667, '791'), (0.4985272427507942, '363'), (0.4879500364742666, '605'), (0.4879500364742666, '325'), (0.4841229182759269, '679'), (0.48304589153964794, '721'), (0.48148148148148096, '321'), (0.4677071733467446, '934'), (0.46291004988627654, '663'), (0.4615384615384598, '665'), (0.458682472293863, '774'), (0.4581709669115191, '417'), (0.457495710997814, '127'), (0.45675013919556895, '178'), (0.4564354645876379, '793'), (0.4564354645876379, '470'), (0.4564354645876379, '468'), (0.4543441112511195, '311'), (0.4472135954999604, '890'), (0.44721359549995826, '63'), (0.44721359549995826, '332'), (0.4472135954999579, '796'), (0.4472135954999579, '312'), (0.4472135954999579, '277'), (0.4472135954999573, '381'), (0.443796691642559, '425'), (0.4409585518440974, '825'), (0.43755650629665205, '653'), (0.4225771273642585, '154'), (0.42008402520840327, '91'), (0.42008402520840327, '498'), (0.42008402520840055, '125'), (0.4193139346887671, '509'), (0.41666666666666613, '569'), (0.40824829046386907, '645'), (0.40824829046386907, '521'), (0.40824829046386907, '472'), (0.40824829046386907, '322'), (0.40824829046386907, '270'), (0.4082482904638648, '199'), (0.4082482904638631, '69'), (0.4082482904638631, '422'), (0.4082482904638631, '188'), (0.4082482904638625, '889'), (0.4082482904638609, '160'), (0.4000661320993201, '413'), (0.4000000000000033, '622'), (0.3973597071195129, '606'), (0.39620290784653006, '239'), (0.39223227027637103, '880'), (0.3859224924939799, '380'), (0.38286875837645057, '280'), (0.3823595564509363, '844'), (0.38188130791298736, '286'), (0.3779644730092272, '936'), (0.3779644730092272, '342'), (0.3726779962499645, '797'), (0.3721042037676255, '532'), (0.3721042037676255, '141'), (0.3721042037676253, '456'), (0.3691751922605913, '727'), (0.3658584816898434, '458'), (0.36295979379571197, '486'), (0.36115755925730797, '786'), (0.36115755925730797, '630'), (0.3572172541558812, '246'), (0.35082320772281256, '871'), (0.35080074795304056, '870'), (0.34641016151377385, '184'), (0.34299717028501714, '903'), (0.3429971702850169, '288'), (0.33942211665106536, '391'), (0.33565855667131145, '389'), (0.3333333333333333, '923'), (0.3333333333333333, '907'), (0.3333333333333333, '676'), (0.3333333333333333, '662'), (0.3333333333333333, '515'), (0.3333333333333333, '501'), (0.3333333333333333, '360'), (0.3333333333333333, '338'), (0.3333333333333333, '331'), (0.3333333333333333, '192'), (0.3333333333333333, '148'), (0.3299560087980459, '10'), (0.3273268353539852, '938'), (0.3273268353539852, '869'), (0.32530002431617777, '546'), (0.3250000000000038, '92'), (0.3207211175227616, '308'), (0.32025630761017265, '432'), (0.3118047822311623, '735'), (0.30779350562554625, '487'), (0.3055050463303894, '347'), (0.30151134457776363, '788'), (0.2889738156210036, '669'), (0.2886751345948129, '151'), (0.2886751345948122, '303'), (0.2842676218074789, '497'), (0.2793721183078316, '158'), (0.27735009811261485, '143'), (0.27735009811261413, '918'), (0.27735009811261413, '836'), (0.27735009811261413, '827'), (0.27216552697590735, '902'), (0.26919095102908275, '699'), (0.26111648393354686, '624'), (0.2581988897471611, '684'), (0.2581988897471611, '397'), (0.2531848417709163, '407'), (0.2508010273650078, '846'), (0.25000000000000444, '536'), (0.25000000000000233, '99'), (0.2499999999999989, '77'), (0.249999999999998, '74'), (0.24999999999999734, '506'), (0.2425356250363332, '834'), (0.2425356250363332, '710'), (0.2425356250363332, '554'), (0.2425356250363332, '292'), (0.24253562503633302, '2'), (0.2401922307076307, '798'), (0.23570226039551587, '853'), (0.23570226039551587, '446'), (0.23145502494313794, '250'), (0.22821773229381945, '610'), (0.22430015382565058, '301'), (0.21926450482675694, '268'), (0.21821789023599533, '597'), (0.2182178902359924, '922'), (0.2182178902359924, '189'), (0.21520701346890458, '334'), (0.2149848538733786, '393'), (0.21004201260419925, '87'), (0.2080625946441211, '533'), (0.204124145231931, '851'), (0.2036532699906392, '130'), (0.20365326999063496, '592'), (0.2025478734167316, '276'), (0.1980295085953336, '345'), (0.19641855032959446, '279'), (0.1956521739130431, '651'), (0.1936491673103722, '924'), (0.19364916731036474, '455'), (0.19245008972987526, '724'), (0.19245008972987526, '423'), (0.19245008972987526, '265'), (0.19245008972987526, '209'), (0.19245008972987526, '126'), (0.19094065395649368, '430'), (0.18898223650461526, '146'), (0.18898223650461524, '650'), (0.18898223650461435, '372'), (0.18898223650461435, '183'), (0.18898223650461402, '598'), (0.18835264373920713, '18'), (0.18681617943926906, '424'), (0.18569533817705197, '267'), (0.18406999323455603, '452'), (0.1788854381999851, '524'), (0.17407765595569785, '576'), (0.17173551629643818, '293'), (0.17149858514250685, '940'), (0.17078251276599318, '79'), (0.16770509831248454, '298'), (0.16666666666666888, '929'), (0.16666666666666863, '54'), (0.16666666666666563, '216'), (0.16666666666666563, '21'), (0.1663073934170612, '144'), (0.1624591083221647, '749'), (0.16183009509158597, '128'), (0.16151457061744964, '815'), (0.15907119074394488, '660'), (0.156249999999997, '804'), (0.1507556722888797, '313'), (0.14869042853329528, '90'), (0.14142135623730764, '666'), (0.13608276348795434, '529'), (0.1336306209562141, '835'), (0.13245323570650439, '490'), (0.13235294117647012, '6'), (0.1316077808820133, '457'), (0.1313064328597235, '479'), (0.1252175806694513, '378'), (0.12499999999999921, '535'), (0.11470786693528405, '747'), (0.11322770341445595, '295'), (0.11000868523905495, '537'), (0.10846522890933111, '399'), (0.10821091383745131, '919'), (0.10540925533894598, '387'), (0.10482848367219334, '875'), (0.10259783520851541, '123'), (0.10050378152592103, '23'), (0.09782319760890644, '916'), (0.09128709291752972, '643'), (0.09128709291752526, '840'), (0.09007546982220971, '913'), (0.08035073760083633, '62'), (0.08006407690254459, '761'), (0.07856742013184013, '698'), (0.07434521426664757, '271'), (0.07027283689263066, '548'), (0.06250000000000083, '694'), (0.06231769528497727, '296'), (0.060192926542885467, '11'), (0.05388159060803127, '648'), (0.05083285677753489, '34'), (0.04928640580901443, '193'), (0.04662524041201693, '833'), (0.03724194613619294, '326'), (0.0322748612183943, '379'), (0.027410122234340577, '95'), (0.021006132059822987, '13')]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, the meat of user-based collaborative filtering - we go through all of the movies that everyone else rated, and weight them based not just on the rating but also on the similarity of the user who rated them.\n",
      "\n",
      "For every movie rating we encounter, we compute a score that is the product of the rating and the correlation with the user who rated it. We sum up all of the scores for each movie, and then divide the sum by the total user similarities for users who rated that movie in order to create a weighted average. That is, for a data set of $n$ ratings:\n",
      "\n",
      "$Score = \\dfrac{\\sum_i^n \\rho_i r_i}{\\sum_i^n \\rho_i}$\n",
      "\n",
      "Where $Score$ is the final score of the movie recommendation, $\\rho_i$ is the Pearson Correalation Coefficient for the user that generated recommendation $i$, and $r_i$ is the rating from that user.\n",
      "\n",
      "**Class discussion point**: Pearson Correlation is only used here as an example; any distance metric may be used in its place. What other distance metrics do you remember, and which ones might be a good fit for user based collaborative filtering?\n",
      "\n",
      "Finally, we sort the result to produce our list of recommendations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getRecommendationCandidates(targetUser, similarUsers, allRatings, numRecommendations):\n",
      "    candidateTotals = {}\n",
      "    for user in similarUsers:\n",
      "        # Get this user's similarity (from -1 to 1)\n",
      "        userSimilarity = user[0]\n",
      "        # Go through this user's ratings\n",
      "        userID = user[1]\n",
      "        for movie in allRatings[userID]:\n",
      "            # Don't recommend stuff I already rated\n",
      "            if allRatings[targetUser].has_key(movie):\n",
      "                continue\n",
      "            # Score the movie based on the rating, penalizing bad ratings\n",
      "            rating = allRatings[userID][movie]\n",
      "            # Multiply by the Pearson correlation with this user\n",
      "            score = rating * userSimilarity\n",
      "            # Update this candidate's total score and total user similarity\n",
      "            if (not(candidateTotals.has_key(movie))):\n",
      "                sources = [(rating, userSimilarity, score)]\n",
      "                candidateTotals[movie] = (score, userSimilarity, \n",
      "                    sources)\n",
      "            else:\n",
      "                (prevScore, prevSim, sources) = candidateTotals[movie];\n",
      "                sources.append((rating, userSimilarity, score))\n",
      "                candidateTotals[movie] = (score + prevScore, \n",
      "                    userSimilarity + prevSim, sources)\n",
      "    \n",
      "    # Compute the averages\n",
      "    recommendationCandidates = []\n",
      "\n",
      "    for candidate in candidateTotals:\n",
      "        (totalScore, totalSim, sourceRatings) = candidateTotals[candidate]\n",
      "        recommendationCandidates.append((totalScore / totalSim, totalScore, \n",
      "            candidate, sourceRatings))\n",
      "        \n",
      "    recommendationCandidates.sort(reverse=True)\n",
      "    \n",
      "    return recommendationCandidates[0 : numRecommendations]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "getRecommendationCandidates('9', similarUsers, ratings, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(5.0,\n",
        "  5.811330892662636,\n",
        "  'Faust (1994)',\n",
        "  [(5.0, 0.6622661785325219, 3.3113308926626095),\n",
        "   (5.0, 0.5000000000000053, 2.5000000000000266)]),\n",
        " (5.0, 5.0, 'Letter From Death Row, A (1998)', [(5.0, 1.0, 5.0)]),\n",
        " (5.0,\n",
        "  4.8536267169707425,\n",
        "  'Swept from the Sea (1997)',\n",
        "  [(5.0, 0.9707253433941485, 4.8536267169707425)]),\n",
        " (5.0,\n",
        "  4.84122918275928,\n",
        "  'Twisted (1996)',\n",
        "  [(5.0, 0.968245836551856, 4.84122918275928)]),\n",
        " (5.0,\n",
        "  3.9144719370393357,\n",
        "  'A Chef in Love (1996)',\n",
        "  [(5.0, 0.7205766921228899, 3.6028834606144495),\n",
        "   (5.0, 0.06231769528497727, 0.31158847642488635)]),\n",
        " (5.0,\n",
        "  3.868844097596101,\n",
        "  'Santa with Muscles (1996)',\n",
        "  [(5.0, 0.5773502691896258, 2.886751345948129),\n",
        "   (5.0, 0.19641855032959446, 0.9820927516479723)]),\n",
        " (5.0,\n",
        "  3.4449111825231027,\n",
        "  'Star Kid (1997)',\n",
        "  [(5.0, 0.5000000000000053, 2.5000000000000266),\n",
        "   (5.0, 0.18898223650461526, 0.9449111825230763)]),\n",
        " (5.0,\n",
        "  2.9107310818591237,\n",
        "  'Prefontaine (1997)',\n",
        "  [(5.0, 0.3721042037676255, 1.8605210188381274),\n",
        "   (5.0, 0.21004201260419925, 1.0502100630209963)]),\n",
        " (5.0,\n",
        "  2.6111648393354674,\n",
        "  'Mondo (1996)',\n",
        "  [(5.0, 0.5222329678670935, 2.6111648393354674)]),\n",
        " (5.0,\n",
        "  2.5000000000000266,\n",
        "  'Leading Man, The (1996)',\n",
        "  [(5.0, 0.5000000000000053, 2.5000000000000266)])]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interestingly, those final scores for each movie (the first element of each result above) can be interepreted as the *user's predicted rating for that movie.*\n",
      "\n",
      "Note that quite a few movies ended up with a predicted rating of 5.0, so we put the sum of the individual rating scores as the second element of the results. This caused our sorting function to weight movies with more ratings from more similar users to bubble up to the top, breaking ties in a meaningful way.\n",
      "\n",
      "###The Challenges of Collaborative Filtering\n",
      "\n",
      "It's important to note that this is just one way to implement user-based collaborative filtering; finding ways of making it better is a constant area of research. Only user 9 knows if these are really good recommendations, but I bet they can be better. All user-based CF really means is \"recommend stuff that other users like me liked.\" You don't have to use the scoring function, or even the algorithm, cited above.\n",
      "\n",
      "We printed out not only the predicted ratings, total scores, and titles of each recommendation, but also the individual ratings that went into each one. You can see that they are all only based on one or two ratings, which leads to some spurious results such as \"Santa with Muscles\" that a couple of users rated 5.0. Collaborative filtering requires a lot of data to work well, and clearly we don't have enough in our data set of 100,000 ratings. How much data you need isn't just a function of the number of ratings - the number of users and the number of items being rated also matters. In practice, you need to dive into the results to figure out how to make them better.\n",
      "\n",
      "Some things you might notice is that ratings from users with low correlations influence the results a lot, due to the small number of users who rated some subset of the same movies that user 9 rated. Enforcing a higher threshold on user similarity could have interesting results. Also, perhaps a 5-point rating scale isn't enough for this small data set - using a 10-point scale, or allowing fractional ratings, would provide more information to the algorithm.\n",
      "\n",
      "Fundamentally, we should take Pearson Correlation Coefficients with a very large grain of salt when the number of data points being compared is small. Should we even consider relationships between users with only one item in common?\n",
      "\n",
      "In the code above, we discard data from users with an inverse correlation. But, given how desperate we are for information, perhaps we could interpret low ratings from inversely-correlated users as potential high ratings for myself?\n",
      "\n",
      "Does it even make sense to give any weight at all to movies rated 1.0 from users who are correlated to me? Perhaps low ratings should have a negative contribution to the score.\n",
      "\n",
      "The answers to these questions depend on the nature of your data and of your users, and what you're trying to achieve.\n",
      "\n",
      "**Discussion point**: \"Social recommenders\" measure user similarity as a function of your distance in a social graph, instead of your distance in implicit or explicit behavior data. How do you think that would work? \n",
      "\n",
      "###Class Assignment:\n",
      "\n",
      "Modify the code above to enforce a higher threshold on user-to-user correlation before considering ratings from a given user. Do you think it makes the results better, or worse? Now, try implementing a more complex scoring function than just correlation * rating, that penalizes low ratings and gives a boost to high ratings and/or filters out correlations based on one or two data points. What effect does that have?\n",
      "\n",
      "###Shilling Attacks:\n",
      "The fact that two users were able to get \"Santa with Muscles\" (A 1996 Hulk Hogan movie rated 2/10 on IMDb) into the top results brings up another challenge of collaborative filtering: *shilling attacks* or *recommendation bombing*. It's entirely possible that the two users who rated this movie 5.0 were just trying to be funny. But in real-world systems, people will try to influence recommendations out of malice. For example, they might create several accounts that highly rate some distasteful item as well as a book from an author they don't like. If you don't have a lot of data, it won't take many of these fake accounts to skew your results.\n",
      "\n",
      "There is also research on how to detect shilling attacks and filter them out, but the best way to improve recommendations is to *get more data.* More data trumps algorithmic tweaks every time, and it will drown out these fake signals.\n",
      "\n",
      "**Class discussion point** What techniques would you use to detect malicious user behavior trying to game a recommender system? \n",
      "\n",
      "###Serendipity\n",
      "The user-based CF algorithm above is designed to predict your rating of movies you haven't seen yet, and present the movies you'll rate the highest. But, what if I haven't rated every movie I've already seen? Our algorithm uses total ratings as a tie-breaker, which means more popular movies will get a boost. Sometimes you don't want to recommend movies you've already heard of; you want to introduce users to great movies for them that they've never even heard of. This discovery of new content that's relevant to you is called *serendipitous discovery*. It's what allows recommender systems to level the playing field for new artists, and get users to consume items they never would have known about otherwise.\n",
      "\n",
      "In practice, you want to strike a balance between recommending items the user has already heard of, and new, serendipitous items. If you saw a list of recommendations full of movies you never heard of, you'd probably think the results were just random noise and discard them. This touches on the concept of *recommender trust* - users will be more likely to explore serendipitous items if they see a few recommendations of things they are familiar with and liked, to give them a sense of trust that this recommender system understands their tastes. Achieving that sense of trust isn't something I can give you a formula for - tuning a recommender system is something of an art. You just need to try a lot of things and measure the results.\n",
      "\n",
      "You can work the overall popularity (or *a priori probablility*) of an item into your scoring function, to adjust the level of serendipity your recommender produces.\n",
      "\n",
      "###Implicit vs. Explicit Data\n",
      "What kind of data you are collecting from users also influences your results, and the amount of data you can collect.\n",
      "\n",
      "-*Explicit Data* are ratings input by a user, with the intent of expressing their opinion about a given item. The movie ratings in our example are explicit data. While explicit ratings can contain a lot of information, they require work from the end user, and you'll only get data from your most engaged users. There is also the problem that a person's stated rating might not actually reflect their real interests. Perhaps they are rating \"The Shawshank Redemption\" highly just because they want to look sophisticated, but they actually have no desire to watch it again.\n",
      "-*Implicit Data* is data derived from natural user behavior. Examples include purchases on an online store, or page views on a website. Because this data is generated as users do whatever they do normally, you tend to get a lot more of it. The quality of implicit data can vary; for example, purchase data requires people to \"vote with their wallets\" and therefore contains a strong signal, but page view data can be quite noisy. Implicit data tends to be binary in nature; while an explicit rating might be on a scale from 1 to 5, a user either viewed or purchased something - or didn't. \n",
      "\n",
      "What kind of data to use depends on what you are trying to achieve. If you're recommending movies to watch, basing that on movie ratings makes sense if you have enough data. If you're recommending items to purchase, basing those on purchase data makes sense.\n",
      "\n",
      "**Class discussion point**: Can you think of situations where you might be able to combine several types of data together for a recommender?\n",
      "\n",
      "## Item-to-Item Similarities\n",
      "\n",
      "We can use the same algorithms we used to find similar users to find items that are similar to each other. If you've ever shopped on Amazon.com, their \"people who bought also bought\" feature is an example of item-to-item similarities. Let's try it out. For user based CF, we started with a list of all the users and the movies they rated. We'll flip this around, and create a list of all the movies and the user ratings associated with them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def buildUsersByItem(ratings):\n",
      "    usersByItem = {}\n",
      "    for user in ratings:\n",
      "        for movie in ratings[user]:\n",
      "            if (not(usersByItem.has_key(movie))):\n",
      "                usersByItem[movie] = {}\n",
      "            usersByItem[movie][user] = ratings[user][movie]\n",
      "    return usersByItem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usersByItem = buildUsersByItem(ratings)\n",
      "print usersByItem['Star Wars (1977)']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'344': 5.0, '345': 5.0, '346': 5.0, '347': 5.0, '340': 4.0, '343': 5.0, '815': 5.0, '719': 2.0, '717': 4.0, '716': 5.0, '715': 5.0, '714': 5.0, '712': 4.0, '711': 4.0, '710': 4.0, '422': 4.0, '619': 4.0, '424': 3.0, '298': 5.0, '299': 4.0, '296': 5.0, '297': 5.0, '294': 5.0, '295': 5.0, '292': 4.0, '293': 5.0, '290': 5.0, '291': 5.0, '593': 4.0, '592': 5.0, '595': 5.0, '594': 3.0, '597': 5.0, '596': 5.0, '194': 3.0, '197': 5.0, '192': 4.0, '270': 5.0, '271': 5.0, '272': 4.0, '274': 5.0, '275': 4.0, '276': 5.0, '277': 3.0, '279': 3.0, '524': 4.0, '526': 5.0, '527': 4.0, '521': 4.0, '523': 5.0, '528': 5.0, '447': 5.0, '445': 2.0, '444': 5.0, '108': 4.0, '109': 5.0, '102': 4.0, '103': 5.0, '101': 4.0, '104': 5.0, '902': 5.0, '903': 5.0, '901': 4.0, '907': 4.0, '32': 4.0, '908': 4.0, '30': 3.0, '37': 5.0, '641': 3.0, '643': 4.0, '642': 5.0, '645': 4.0, '644': 4.0, '438': 5.0, '436': 4.0, '437': 5.0, '435': 5.0, '432': 5.0, '433': 5.0, '430': 4.0, '339': 4.0, '334': 5.0, '337': 5.0, '336': 4.0, '330': 5.0, '332': 5.0, '854': 4.0, '850': 5.0, '851': 5.0, '852': 5.0, '748': 5.0, '6': 4.0, '99': 5.0, '91': 5.0, '92': 5.0, '95': 5.0, '94': 5.0, '97': 5.0, '96': 5.0, '741': 5.0, '742': 4.0, '744': 3.0, '745': 2.0, '746': 5.0, '747': 5.0, '555': 5.0, '554': 4.0, '557': 4.0, '551': 2.0, '550': 5.0, '553': 4.0, '552': 4.0, '239': 5.0, '234': 4.0, '235': 5.0, '236': 3.0, '230': 5.0, '231': 4.0, '232': 4.0, '233': 3.0, '1': 5.0, '618': 5.0, '145': 5.0, '141': 4.0, '613': 5.0, '610': 4.0, '148': 5.0, '942': 5.0, '943': 4.0, '940': 4.0, '689': 5.0, '684': 4.0, '686': 4.0, '680': 5.0, '682': 5.0, '623': 5.0, '622': 5.0, '132': 3.0, '130': 5.0, '137': 5.0, '499': 3.0, '494': 5.0, '495': 5.0, '496': 5.0, '497': 5.0, '490': 5.0, '493': 5.0, '25': 5.0, '26': 4.0, '27': 3.0, '20': 3.0, '21': 3.0, '22': 5.0, '23': 4.0, '28': 4.0, '407': 4.0, '406': 5.0, '405': 5.0, '938': 5.0, '403': 5.0, '402': 4.0, '401': 1.0, '933': 4.0, '931': 3.0, '930': 2.0, '937': 5.0, '629': 5.0, '409': 5.0, '934': 5.0, '379': 4.0, '378': 4.0, '371': 4.0, '370': 4.0, '373': 5.0, '826': 5.0, '374': 3.0, '823': 5.0, '708': 5.0, '709': 5.0, '704': 5.0, '705': 4.0, '706': 5.0, '700': 5.0, '144': 5.0, '703': 5.0, '393': 5.0, '392': 5.0, '391': 4.0, '89': 5.0, '397': 5.0, '395': 5.0, '394': 5.0, '82': 5.0, '83': 3.0, '80': 3.0, '398': 5.0, '87': 5.0, '85': 5.0, '797': 5.0, '796': 5.0, '795': 3.0, '794': 5.0, '793': 5.0, '791': 5.0, '790': 4.0, '799': 4.0, '798': 5.0, '7': 5.0, '586': 4.0, '584': 4.0, '582': 5.0, '580': 5.0, '581': 4.0, '588': 5.0, '245': 4.0, '244': 5.0, '247': 5.0, '246': 5.0, '921': 4.0, '249': 4.0, '248': 5.0, '513': 5.0, '512': 5.0, '514': 5.0, '517': 5.0, '516': 5.0, '458': 2.0, '459': 4.0, '621': 5.0, '620': 4.0, '625': 5.0, '624': 5.0, '450': 5.0, '452': 5.0, '453': 5.0, '454': 4.0, '455': 5.0, '456': 4.0, '457': 5.0, '178': 5.0, '177': 5.0, '176': 5.0, '175': 5.0, '174': 4.0, '198': 5.0, '182': 5.0, '183': 2.0, '654': 5.0, '2': 5.0, '653': 5.0, '184': 4.0, '185': 4.0, '188': 4.0, '189': 5.0, '658': 4.0, '659': 3.0, '650': 5.0, '869': 4.0, '10': 5.0, '13': 5.0, '12': 4.0, '15': 5.0, '14': 5.0, '18': 4.0, '862': 5.0, '864': 5.0, '867': 5.0, '883': 4.0, '882': 5.0, '881': 3.0, '880': 5.0, '887': 5.0, '886': 5.0, '885': 3.0, '889': 4.0, '322': 5.0, '323': 5.0, '320': 4.0, '321': 4.0, '326': 5.0, '327': 3.0, '324': 5.0, '325': 5.0, '328': 4.0, '329': 4.0, '562': 5.0, '201': 4.0, '200': 5.0, '203': 5.0, '776': 5.0, '771': 4.0, '770': 3.0, '773': 5.0, '209': 5.0, '779': 5.0, '77': 4.0, '72': 2.0, '71': 3.0, '70': 4.0, '655': 4.0, '567': 1.0, '79': 4.0, '359': 5.0, '669': 5.0, '668': 5.0, '666': 3.0, '665': 4.0, '664': 5.0, '663': 5.0, '662': 3.0, '661': 5.0, '660': 4.0, '693': 3.0, '691': 4.0, '697': 5.0, '694': 5.0, '698': 5.0, '699': 3.0, '542': 4.0, '540': 5.0, '541': 5.0, '546': 5.0, '545': 5.0, '8': 5.0, '548': 5.0, '549': 5.0, '868': 5.0, '120': 4.0, '121': 5.0, '123': 3.0, '124': 3.0, '125': 5.0, '127': 4.0, '128': 4.0, '214': 3.0, '563': 5.0, '929': 4.0, '416': 5.0, '417': 3.0, '411': 5.0, '413': 5.0, '498': 4.0, '922': 5.0, '923': 5.0, '924': 5.0, '419': 5.0, '318': 2.0, '313': 5.0, '312': 5.0, '311': 5.0, '310': 5.0, '316': 1.0, '832': 3.0, '833': 2.0, '830': 5.0, '831': 5.0, '834': 5.0, '835': 4.0, '838': 5.0, '839': 5.0, '368': 4.0, '369': 5.0, '367': 5.0, '363': 5.0, '360': 4.0, '361': 5.0, '380': 4.0, '381': 5.0, '382': 1.0, '385': 1.0, '386': 4.0, '387': 5.0, '389': 5.0, '785': 5.0, '786': 4.0, '780': 5.0, '781': 5.0, '782': 3.0, '789': 5.0, '579': 5.0, '573': 4.0, '577': 4.0, '576': 4.0, '575': 2.0, '60': 5.0, '62': 5.0, '63': 4.0, '64': 5.0, '65': 5.0, '66': 5.0, '68': 5.0, '69': 5.0, '250': 5.0, '251': 5.0, '256': 4.0, '257': 5.0, '254': 5.0, '154': 5.0, '157': 4.0, '730': 4.0, '735': 5.0, '734': 4.0, '508': 5.0, '736': 3.0, '506': 5.0, '738': 5.0, '504': 3.0, '505': 3.0, '503': 5.0, '500': 3.0, '630': 3.0, '632': 5.0, '633': 4.0, '634': 4.0, '468': 5.0, '637': 4.0, '465': 4.0, '464': 4.0, '467': 4.0, '466': 5.0, '461': 3.0, '463': 4.0, '169': 5.0, '160': 4.0, '161': 2.0, '162': 5.0, '936': 4.0, '878': 4.0, '879': 4.0, '875': 5.0, '870': 3.0, '871': 5.0, '9': 5.0, '890': 5.0, '891': 4.0, '892': 5.0, '893': 5.0, '894': 4.0, '895': 5.0, '896': 5.0, '897': 5.0, '899': 5.0, '649': 4.0, '648': 5.0, '354': 4.0, '352': 5.0, '350': 5.0, '800': 4.0, '807': 5.0, '806': 5.0, '805': 4.0, '804': 4.0, '216': 4.0, '217': 1.0, '768': 4.0, '215': 5.0, '213': 5.0, '210': 5.0, '763': 4.0, '760': 3.0, '761': 5.0, '766': 4.0, '764': 3.0, '765': 2.0, '288': 4.0, '4': 5.0, '280': 3.0, '283': 5.0, '287': 5.0, '286': 4.0, '678': 4.0, '679': 5.0, '674': 4.0, '676': 5.0, '671': 5.0, '672': 3.0, '263': 5.0, '262': 2.0, '267': 5.0, '265': 2.0, '269': 3.0, '268': 5.0, '59': 5.0, '58': 4.0, '55': 4.0, '54': 5.0, '57': 5.0, '56': 5.0, '51': 5.0, '53': 4.0, '537': 4.0, '536': 5.0, '535': 5.0, '533': 5.0, '530': 4.0, '539': 3.0, '538': 5.0, '774': 4.0, '115': 5.0, '117': 5.0, '116': 3.0, '113': 5.0, '253': 4.0, '119': 5.0, '429': 5.0, '919': 3.0, '421': 5.0, '917': 3.0, '916': 5.0, '425': 5.0, '910': 5.0, '913': 4.0, '426': 4.0, '308': 5.0, '301': 5.0, '303': 5.0, '305': 5.0, '307': 5.0, '825': 4.0, '847': 4.0, '846': 5.0, '844': 5.0, '843': 3.0, '840': 4.0, '848': 5.0, '753': 4.0, '569': 5.0, '751': 5.0, '757': 4.0, '756': 4.0, '560': 5.0, '561': 3.0, '759': 4.0, '758': 4.0, '564': 4.0, '566': 2.0, '701': 5.0, '739': 4.0, '507': 5.0, '227': 4.0, '222': 4.0, '221': 4.0, '727': 4.0, '723': 4.0, '721': 5.0, '151': 5.0, '150': 5.0, '153': 1.0, '606': 5.0, '601': 5.0, '600': 4.0, '603': 5.0, '602': 5.0, '158': 4.0, '608': 1.0, '399': 3.0, '749': 5.0, '48': 4.0, '49': 1.0, '46': 4.0, '44': 5.0, '45': 5.0, '42': 5.0, '43': 4.0, '41': 5.0, '638': 4.0, '5': 4.0, '488': 4.0, '487': 4.0, '486': 5.0, '484': 5.0, '483': 5.0, '482': 4.0, '481': 4.0, '480': 4.0, '509': 5.0, '472': 5.0, '470': 5.0, '471': 3.0, '474': 5.0, '475': 5.0, '478': 3.0, '479': 4.0}\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our routine to find similar items is exactly the same as the one we wrote earlier for finding similar users; we just used the word \"item\" in the place of \"user\":"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def findSimilarItems(targetItemID, allRatings):\n",
      "    similarItems = []\n",
      "    targetItemRatings = allRatings[targetItemID]\n",
      "    for item in allRatings:\n",
      "        # Exclude myself:\n",
      "        if (item != targetItemID):\n",
      "            itemSimilarity = 0\n",
      "            # Find mutual users (if any)\n",
      "            targetItemRatingsArray = []\n",
      "            otherItemRatingsArray = []\n",
      "            for user in targetItemRatings:\n",
      "                if allRatings[item].has_key(user):\n",
      "                    targetItemRatingsArray.append(\n",
      "                        float(targetItemRatings[user]))\n",
      "                    otherItemRatingsArray.append(\n",
      "                        float(allRatings[item][user]))\n",
      "        \n",
      "            # Compute the similarity between these users\n",
      "            itemSimilarity = pearson(targetItemRatingsArray, \n",
      "                otherItemRatingsArray)\n",
      "            # If there is a positive correlation:\n",
      "            if itemSimilarity > 0.0:\n",
      "                similarItems.append((itemSimilarity, item))\n",
      "    \n",
      "    # Sort the list by similarity\n",
      "    similarItems.sort(reverse=True)\n",
      "    return similarItems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it out!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarities = findSimilarItems('Star Wars (1977)', usersByItem)\n",
      "print similarities[1:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(1.0000000000000013, 'No Escape (1994)'), (1.0000000000000013, 'Commandments (1997)'), (1.000000000000001, 'Designated Mourner, The (1997)'), (1.0000000000000007, 'Hollow Reed (1996)'), (1.0, 'Twisted (1996)'), (1.0, 'Stripes (1981)'), (1.0, 'Scarlet Letter, The (1926)'), (1.0, 'Safe Passage (1994)'), (1.0, 'Outlaw, The (1943)'), (1.0, 'Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991)'), (1.0, 'Mondo (1996)'), (1.0, 'Line King: Al Hirschfeld, The (1996)'), (1.0, 'Hurricane Streets (1998)'), (1.0, 'Good Man in Africa, A (1994)'), (1.0, 'Golden Earrings (1947)'), (1.0, 'Full Speed (1996)'), (1.0, \"Ed's Next Move (1996)\"), (1.0, 'Beans of Egypt, Maine, The (1994)'), (0.9999999999999987, 'Maya Lin: A Strong Clear Vision (1994)'), (0.9999999999999959, 'Man of the Year (1995)'), (0.9999999999999947, 'Last Time I Saw Paris, The (1954)'), (0.9684959969581862, 'Albino Alligator (1996)'), (0.9622504486493763, 'Angel Baby (1995)'), (0.9271726499455306, 'Prisoner of the Mountains (Kavkazsky Plennik) (1996)'), (0.9233805168766379, 'Love in the Afternoon (1957)'), (0.872871560943971, \"'Til There Was You (1997)\"), (0.8685990362153774, 'A Chef in Love (1996)'), (0.8660254037844402, 'Guantanamera (1994)'), (0.866025403784439, 'Substance of Fire, The (1996)'), (0.866025403784439, 'Quiet Room, The (1996)'), (0.8660254037844378, 'Rhyme & Reason (1997)'), (0.8660254037844355, 'Wedding Gift, The (1994)'), (0.8181818181818182, 'Zeus and Roxanne (1997)'), (0.8040844011283466, 'Dream With the Fishes (1997)'), (0.7905694150420947, 'Paradise Road (1997)'), (0.7717436331412897, 'Unhook the Stars (1996)'), (0.7559289460184573, 'Hearts and Minds (1996)'), (0.7559289460184551, 'Ciao, Professore! (1993)'), (0.75, 'That Old Feeling (1997)'), (0.75, 'Bliss (1997)'), (0.7479814223788697, 'Empire Strikes Back, The (1980)'), (0.7231232535460361, 'unknown'), (0.7225916412701, 'American Buffalo (1996)'), (0.7071067811865476, 'Wonderful, Horrible Life of Leni Riefenstahl, The (1993)'), (0.7071067811865475, 'City of Angels (1998)'), (0.6725558558876057, 'Return of the Jedi (1983)'), (0.6454972243679021, 'Grace of My Heart (1996)'), (0.6393620130910042, 'Cronos (1992)'), (0.6333118779395084, 'Meet John Doe (1941)')]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There's our small data size at play again - while it's encouraging that \"The Empire Strikes Back\" and \"Return of the Jedi\" are near the top, they are being outranked by seemingly unrelated films. Again, the best solution to this problem is more data, but algorithmic tweaks are possible.\n",
      "\n",
      "**Class discussion point**: Do you think Pearson Correlation Coefficient is the best similarity function to use in this context? \n",
      "\n",
      "We're also seeing problems from using Pearson Correlation Coefficients from items that only have one or two users in common. Discarding situations where we have low data would be advisable.\n",
      "\n",
      "It is interesting to note that for our algorithm, the top similarity for \"Star Wars\" (\"Cosi\") has \"Star Wars\" as its own top similarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "findSimilarItems('Cosi (1996)', usersByItem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[(1.000000000000004, 'Star Wars (1977)'),\n",
        " (1.0, 'Waiting for Guffman (1996)'),\n",
        " (1.0, 'Thousand Acres, A (1997)'),\n",
        " (1.0, \"Romy and Michele's High School Reunion (1997)\"),\n",
        " (1.0, 'Return of the Jedi (1983)'),\n",
        " (1.0, 'Private Parts (1997)'),\n",
        " (1.0, 'Michael Collins (1996)'),\n",
        " (1.0, 'Love! Valour! Compassion! (1997)'),\n",
        " (1.0, 'Lost Highway (1997)'),\n",
        " (1.0, 'Long Kiss Goodnight, The (1996)'),\n",
        " (1.0, 'L.A. Confidential (1997)'),\n",
        " (1.0, 'Kama Sutra: A Tale of Love (1996)'),\n",
        " (1.0, 'Jackie Brown (1997)'),\n",
        " (1.0, 'Hard Eight (1996)'),\n",
        " (1.0, 'Everyone Says I Love You (1996)'),\n",
        " (1.0, 'Donnie Brasco (1997)'),\n",
        " (1.0, 'Daytrippers, The (1996)'),\n",
        " (1.0, 'Cop Land (1997)'),\n",
        " (1.0, 'Breakdown (1997)'),\n",
        " (1.0, 'Boot, Das (1981)'),\n",
        " (1.0, 'Boogie Nights (1997)'),\n",
        " (1.0, 'Bean (1997)'),\n",
        " (1.0, 'Apt Pupil (1998)'),\n",
        " (1.0, 'Alien: Resurrection (1997)'),\n",
        " (0.8660254037844402, \"Ulee's Gold (1997)\"),\n",
        " (0.8660254037844402, 'Grosse Pointe Blank (1997)'),\n",
        " (0.8660254037844402, 'Chasing Amy (1997)'),\n",
        " (0.8528028654224417, 'Mother (1996)'),\n",
        " (0.7071067811865476, 'English Patient, The (1996)'),\n",
        " (0.5000000000000017, 'Crash (1996)'),\n",
        " (0.5, 'Air Force One (1997)'),\n",
        " (0.49999999999999933, \"Muriel's Wedding (1994)\")]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Hybrid Recommenders and the Cold Start Problem\n",
      "Our similar movies for \"Star Wars\" seem disappointing - there are some good results in there, but a lot seem spurious due to our small data set. (Yes, 100,000 ratings is small when you have over 1,000 items and users in the mix.) We need more information to make better recommendations - but from where?\n",
      "\n",
      "*Hybrid recommenders* combine user behavior data with item attribute data to improve recommendation results. Why do the similar items for Star Wars look wrong to us? We're probably expecting other science fiction films to be on top of the list. Well, the simplest thing we can do is just filter the results by films that are in the sci-fi genre.\n",
      "\n",
      "As it turns out, there is genre information in the MovieLens data set that we can load up:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadMovieGenres():\n",
      "    genreCodes = {}\n",
      "    for line in open('../input/ml-100k/u.genre'):\n",
      "        movieData = line.split('|')\n",
      "        if len(movieData) == 2:\n",
      "            genreCodes[int(movieData[1])] = movieData[0]\n",
      "    \n",
      "    movieGenres = {}\n",
      "    for line in open('../input/ml-100k/u.item'):\n",
      "        movieData = line.split('|')\n",
      "        movieName = movieData[1]\n",
      "        movieGenres[movieName] = {}\n",
      "        for i in range(19):\n",
      "            idx = i + 5\n",
      "            if movieData[idx] == '1':\n",
      "                genreName = genreCodes[i]\n",
      "                movieGenres[movieName][genreName] = 1\n",
      "    return movieGenres\n",
      "\n",
      "genres = loadMovieGenres()\n",
      "print genres['Star Wars (1977)']       "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'Action': 1, 'Romance': 1, 'War': 1, 'Adventure': 1, 'Sci-Fi': 1}\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's write a little function to filter our results down to films that are tagged as sci-fi. Filtering is just one way to apply item attribute data such as film genre; we could also use genre to give a boost to items of the same genre as part of our scoring function, or to films that belong to the user's favorite genres."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filterRecommendations(recommendations, genreFilter, genres):\n",
      "    filteredRecommendations = []\n",
      "    for item in recommendations:\n",
      "        name = item[1]\n",
      "        if genres[name].has_key(genreFilter):\n",
      "            filteredRecommendations.append(item)\n",
      "    return filteredRecommendations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's apply this filter to our similarities for Star Wars, and see if the results look better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "starWarsSims = findSimilarItems('Star Wars (1977)', usersByItem)\n",
      "sciFiSims = filterRecommendations(starWarsSims, 'Sci-Fi', genres)\n",
      "print sciFiSims[0 : 10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(1.0000000000000013, 'No Escape (1994)'), (0.7479814223788697, 'Empire Strikes Back, The (1980)'), (0.6725558558876057, 'Return of the Jedi (1983)'), (0.6054055145966799, 'Dark City (1998)'), (0.599564111820418, 'Ghost in the Shell (Kokaku kidotai) (1995)'), (0.5673070188667113, \"Robert A. Heinlein's The Puppet Masters (1994)\"), (0.45035197502448093, 'Highlander III: The Sorcerer (1994)'), (0.40094080882607086, 'Lawnmower Man 2: Beyond Cyberspace (1996)'), (0.3631354142015637, 'Hellraiser: Bloodline (1996)'), (0.3487184582054387, 'Lost in Space (1998)')]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yes, yes they do. (The MovieLens data set is starting to show its age, by the way. Unfortunately, more current data sets such as from Netflix or IMDb are behind closed doors. You'll just have to get a job there to play with them!)\n",
      "\n",
      "This is just one, very simple example of creating a hybrid recommender. Using information inherent to the items to filter or weight your results can be a powerful technique, especially if your data is sparse. However, it's important to make sure the data you are using is accurate. If a user expects to see a list of sci-fi recommendations and sees \"Steel Magnolias\" in the list, they will lose trust in your recommender.\n",
      "\n",
      "Using item attribute data is also a good approach to the *cold start problem.* How can I make good recommendations for a user that hasn't rated anything yet? Well, if I can get that user to at least tell me something about his or her tastes, like their favorite genre, actor, or director, and I can just present them with a list of movies that fit that description - and then ask them to rate them. If you don't even know that much about the user, just showing them the most popular or well-rated items in your data set is also a good place to start. When you're dealing with a cold-start situation, your objective is to get items in front of the user that they already have consumed and can rate - not to get new items in front of the user to discover.\n",
      "\n",
      "## Item-Based Collaborative Filtering\n",
      "We can take the final step, and use these item-to-item relationships to produce recommendations for an individual user.\n",
      "\n",
      "While user-based CF recommends stuff that people liked me liked, item-based CF recommends stuff that's similar to stuff I liked.\n",
      "\n",
      "This is important for a couple of reasons.\n",
      "\n",
      "- In most systems, there are more users than there are items. As a result, an item-based approach will scale better than a user-based approach. In user-based CF, we need to compute relationships between every user, but in item-based CF, we only compute relationships between every item. If there are fewer items than users, item-based can run faster.\n",
      "- Items don't change much, but people do. A math book will always be a math book, but my interests change all the time. As a result, I can compute item-to-item relationships infrequently, and apply them to my fast-changing interests as expressed by my implicit and explicit ratings in real time.\n",
      "\n",
      "Item-based CF isn't without downsides. Even though the attributes of an item may not change (unlike people,) people tend to outlive items. In a low-data environment, this season's hot new dress might not be on the market long enough to build up meaningful similarities to it. This is another case where hybrid approaches can help - perhaps rolling items up to the brand level or style level would provide more longevity to the items, as well as provide more data per item.\n",
      "\n",
      "We'll illustrate this technique here. First, we'll pre-compute all of the item to item similarities for all of the movies in our data set (this will take a few minutes to run:)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def buildSimilarItemsTable(movieNames, usersByItem):\n",
      "    similarItems = {}\n",
      "    for movie in movieNames:\n",
      "        similarItems[movieNames[movie]] = findSimilarItems(\n",
      "            movieNames[movie], usersByItem)\n",
      "    return similarItems"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Form here, we just look for similar items to the items we have rated. We'll weight them by our rating and similarity score, and sort the results to produce our recommendations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getItemBasedRecommendations(userID, movieNames, usersByItem, userRatings):\n",
      "    # Build lookup table of similar items\n",
      "    similarities = buildSimilarItemsTable(movieNames, usersByItem)\n",
      "    # Get the list of stuff I rated\n",
      "    myRatings = userRatings[userID]\n",
      "    candidates = []\n",
      "    for movie in myRatings:\n",
      "        # Find similar items, weight them by the item similarity and my rating\n",
      "        similarItems = similarities[movie]\n",
      "        for similarItem in similarItems:\n",
      "            # (score, movie name)\n",
      "            candidates.append( (myRatings[movie] * similarItem[0], \n",
      "                similarItem[1]) )\n",
      "    candidates.sort(reverse=True)\n",
      "    return candidates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recommendations = getItemBasedRecommendations('9', movieNames, usersByItem, ratings)\n",
      "print recommendations[0 : 10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(5.0000000000000195, 'Waterworld (1995)'), (5.0000000000000195, 'Telling Lies in America (1997)'), (5.0000000000000195, \"Roseanna's Grave (For Roseanna) (1997)\"), (5.0000000000000195, 'Rent-a-Kid (1995)'), (5.0000000000000195, 'Princess Caraboo (1994)'), (5.0000000000000195, 'Living in Oblivion (1995)'), (5.0000000000000195, 'Last Time I Saw Paris, The (1954)'), (5.0000000000000195, 'Last Time I Saw Paris, The (1954)'), (5.0000000000000195, 'Kim (1950)'), (5.0000000000000195, 'Judgment Night (1993)')]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note how the results are completely different from the user-based approach we used earlier for this user. Which results are better? You'll explore this in your homework assignment!\n",
      "\n",
      "##More to Explore\n",
      "Recommender systems is an area of active research, and collaborative filtering is just one way to do it. If you'd like to learn more, some hot areas are:\n",
      "\n",
      "- Probabalistic Latent Semantic Analysis, and latent factors in general\n",
      "- Mixture of Experts approaches\n",
      "- Social Recommenders\n",
      "\n",
      "Also, check out Apache Mahout for a scalable framework that makes experimenting with new recommender algorithms easy.\n",
      "\n",
      "## Links\n",
      "- [Walkthrough of Hulu's Recommendation System](http://tech.hulu.com/blog/2011/09/19/recommendation-system/)\n",
      "- [Xavier Amatriain @ Netflix](http://www.cikm2013.org/slides/xavier.pdf)\n",
      "- [Yhat Beer Recommendation](http://nbviewer.ipython.org/gist/glamp/20a18d52c539b87de2af)\n",
      "- [Mendeley RecSys](http://mendeley.github.io/mrec/)\n",
      "- [Crab RecSys](http://muricoca.github.io/crab/tutorial.html#introducing-recommendation-engines)\n",
      "\n",
      "##Homework\n",
      "\n",
      "We produced predicted ratings for a user with user-based and item-based collaborative filtering, after training these algorithms on other users' ratings.\n",
      "\n",
      "We can use cross-validation to measure the accuracy of these rating predictions. Use k-fold cross validation to measure the average Mean Absolute Error from user-based and item-based CF. Which performs better for this data set?\n",
      "\n",
      "*BONUS - try and improve on these MAE values by doing something simple. Some ideas:\n",
      "\n",
      "- Apply thresholds for correlations or ratings\n",
      "- Use a different similarity metric than Pearson\n",
      "- Use more data - larger data sets are available from grouplens.org that you can sample from.\n",
      "\n",
      "Let's see who can come up with the lowest MAE!\n",
      "\n",
      "Something to think about: does MAE really measure what we're trying to achieve with a recommender system?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}